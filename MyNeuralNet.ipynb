{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrYsu4kwgsfmmsdh2X7cNG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solmvz/MLActivities/blob/main/MyNeuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ySzQMtN6kpc5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import deepcopy as copy\n",
        "from sklearn.metrics import accuracy_score\n",
        "# round_off numpy output to 3 decimal places\n",
        "np.set_printoptions(precision=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNeuralNet(object):\n",
        "  \n",
        "  def __init__(self, X, y, layers, m, lr=0.01, factor=0.01):\n",
        "    \"\"\"\n",
        "    Our init values - X (the input), y (target values),\n",
        "    learning_rate, factor (determines how small the parameters are)\n",
        "    \n",
        "    \"\"\"\n",
        "    self.X = X\n",
        "    self.layers = layers\n",
        "    self.y = y\n",
        "    self.lr = lr\n",
        "    self.factor = factor\n",
        "    self.m = m\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "  def parameters_init(self):\n",
        "      \"\"\"\n",
        "        Returns:\n",
        "        parameters -- python dictionary containing initial parameter values:\n",
        "        W1 - weight matrix of shape (n1, n0),\n",
        "        b1 - bias vector of shape (n1, 1),\n",
        "        W2 - weight matrix of shape (n2, n1),\n",
        "        b2 - bias vector of shape (n2, 1), where,\n",
        "        n0 - number of the neurons at the input,\n",
        "        n1 - number of neurons at the hidden layer, and\n",
        "        n2 - number of units at the last/output layer.\n",
        "\n",
        "      \"\"\"\n",
        "      # num of neurons in each layer\n",
        "      n0 , n1, n2 = self.layers\n",
        "\n",
        "      np.random.seed(3)\n",
        "\n",
        "      # LAYER 1\n",
        "      w1 = np.random.randn(n1, n0) * self.factor\n",
        "      b1 = np.zeros((n1, 1))\n",
        "\n",
        "      # LAYER 2\n",
        "      w2 = np.random.randn(n2, n1) * self.factor\n",
        "      b2 = np.zeros((n2, 1))\n",
        "\n",
        "      parameters = {\n",
        "          \"w1\": w1,\n",
        "          \"b1\": b1,\n",
        "          \"w2\": w2,\n",
        "          \"b2\": b2\n",
        "      }\n",
        "      \n",
        "      return parameters\n",
        "\n",
        "  def forward_propagation(self, parameters):\n",
        "\n",
        "      \"\"\"\n",
        "        Returns:\n",
        "        yhat - model output on one forward pass for all the training examples,\n",
        "        layer_ouputs - a dictionary containing model outputs at each layer.\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      w1 = parameters[\"w1\"]\n",
        "     # print(\"w1 shape: \", w1.shape)\n",
        "\n",
        "      b1 = parameters[\"b1\"]\n",
        "     # print(\"b1 shape: \", b1.shape)\n",
        "\n",
        "      w2 = parameters[\"w2\"]\n",
        "     # print(\"w2 shape: \", w2.shape)\n",
        "\n",
        "      b2 = parameters[\"b2\"]\n",
        "     # print(\"b2 shape: \", b2.shape)\n",
        "\n",
        "\n",
        "      z1 = np.dot(w1, self.X) + b1\n",
        "     # print(\"z1 shape: \", z1.shape)\n",
        "      f1 = self.sigmoid(z1)\n",
        "     # print(\"f1 shape: \", f1.shape)\n",
        "      \n",
        "      z2 = np.dot(w2, f1) + b2\n",
        "     # print(\"z2 shape: \", z2.shape)\n",
        "\n",
        "      yhat = self.sigmoid(z2)\n",
        "     # print(\"yhat shape:\", yhat.shape)\n",
        "\n",
        "      assert(yhat.shape == (1, self.X.shape[1]))\n",
        "\n",
        "      layer_outputs = {\n",
        "          \"z1\": z1,\n",
        "          \"f1\": f1,\n",
        "          \"z2\": z2,\n",
        "          \"yhat\": yhat\n",
        "      }\n",
        "\n",
        "      return yhat, layer_outputs\n",
        "\n",
        "  def compute_cost(self, yhat, y):\n",
        "      \"\"\"\n",
        "        Computes the cross-entropy cost of approximation\n",
        "        Arguments:\n",
        "        y_hat - Output of forward propagation. Its order is (1, # trainingexamples)\n",
        "        y - vector of true values. Dimension is (1, # training examples)\n",
        "        Returns:\n",
        "        the cost\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      cost = -np.sum(np.multiply(y, np.log(yhat)) + np.multiply(1-y, np.log(1-yhat)))/self.m\n",
        "\n",
        "      #squeeze and make sure cost is the expected dimension\n",
        "      cost = float(np.squeeze(cost))\n",
        "\n",
        "      return cost\n",
        "  \n",
        "  def backward_propagation(self, parameters, layers_output):\n",
        "\n",
        "      \"\"\"\n",
        "        Arguments:\n",
        "        parameters -- python dictionary containing our parameters \n",
        "        layers_output -- a dictionary containing \"Z1\", \"f1\", \"Z2\" and \"yhat\".\n",
        "    \n",
        "        Returns:\n",
        "        gradients -- python dictionary containing our gradients with respect to different parameters\n",
        "\n",
        "      \"\"\"\n",
        "      X = self.X\n",
        "\n",
        "      w1 = parameters[\"w1\"]\n",
        "      w2 = parameters[\"w2\"]\n",
        "\n",
        "      f1 = layers_output[\"f1\"]\n",
        "      yhat = layers_output[\"yhat\"]\n",
        "\n",
        "      dz2 = yhat - self.y\n",
        "      dw2 = np.dot(dz2, f1.T)/self.m\n",
        "      # sum along columns (axis=1)\n",
        "      # keepdims= Truethe axes which are reduced are left in \n",
        "      # the result as dimensions with size one. This allows for correct\n",
        "      # array broadcasting\n",
        "      db2 = 1/self.m * np.sum(dz2, axis=1, keepdims=True)\n",
        "\n",
        "      dz1 = np.dot(w2.T ,dz2) * (1-f1)*f1\n",
        "      dw1 = np.dot(dz1, X.T)/self.m\n",
        "      db1 = 1/self.m *np.sum(dz1, axis=1, keepdims=True)\n",
        "\n",
        "      gradients = {\n",
        "          \"dw1\": dw1,\n",
        "          \"db1\": db1,\n",
        "          \"dw2\": dw2,\n",
        "          \"db2\": db2\n",
        "      }\n",
        "\n",
        "      return gradients\n",
        "\n",
        "  def update_parameters(self, parameters, gradients):\n",
        "\n",
        "    \"\"\"\n",
        "        Updates parameters using the gradient descent update rule discussed above\n",
        "        Arguments:\n",
        "        parameters -- python dictionary containing your parameters \n",
        "        gradients -- python dictionary containing your gradients \n",
        "        Returns:\n",
        "        parameters -- python dictionary containing your updated parameters \n",
        "      \n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve a copy of each parameter from the dictionary \"parameters\".\n",
        "    w1 = copy(parameters[\"w1\"])\n",
        "    b1 = copy(parameters[\"b1\"])\n",
        "    w2 = copy(parameters[\"w2\"])\n",
        "    b2 = copy(parameters[\"b2\"])\n",
        "        \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    dw1 = copy(gradients[\"dw1\"])\n",
        "    db1 = copy(gradients[\"db1\"])\n",
        "    dw2 = copy(gradients[\"dw2\"])\n",
        "    db2 = copy(gradients[\"db2\"])\n",
        "\n",
        "    # update parameters \n",
        "    # hidden-input layer\n",
        "    w1 = w1 - self.lr*dw1\n",
        "    b1 = b1 - self.lr*db1\n",
        "    # output-hidden layer\n",
        "    w2 = w2 - self.lr*dw2\n",
        "    b2 = b2 - self.lr*db2\n",
        "\n",
        "    parameters = {\n",
        "        \"w1\": w1,\n",
        "        \"b1\": b1,\n",
        "        \"w2\": w2,\n",
        "        \"b2\": b2\n",
        "    }\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "izdLItWilB-c"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "6-lAPgyrvULx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(n_iter, X, y, layers, n_examples, lr, factor):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        n_iterations - number of training iterations,\n",
        "        X - the input data (feature data),\n",
        "        y - target/true values,\n",
        "        layers - a tuple with the # neurons for each layer\n",
        "        lr - the learning rate,\n",
        "        factor -  user-defined value that determines how big initialized parameters are\n",
        "    \n",
        "    \"\"\"\n",
        "    s = MyNeuralNet(X, y, layers, n_examples, lr, factor)\n",
        "\n",
        "    parameters = s.parameters_init()\n",
        "\n",
        "    for iter in range(n_iter+1):\n",
        "      # perform forward propagation\n",
        "      y_hat, layers_output = s.forward_propagation(parameters)\n",
        "      # compute cost\n",
        "      cost = round(s.compute_cost(y_hat, y), 3)\n",
        "      # compute gradients and update parameters\n",
        "      gradients = s.backward_propagation(parameters, layers_output)\n",
        "      parameters = s.update_parameters(parameters, gradients)\n",
        "      # make predictions and compute accuracy\n",
        "      predictions = np.select(\n",
        "          [y_hat<0.5, y_hat>=0.5],\n",
        "          [0, 1])\n",
        "      accuracy = round(accuracy_score(np.squeeze(y), np.squeeze(predictions)), 3)\n",
        "      if iter%1000==0 or iter==n_iter:\n",
        "            print(\"iteration\",iter,\"cost:-->\", cost, \"accuracy-->\", accuracy)\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "Xq9bLkLjvWmb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://kipronokoech.github.io/assets/datasets/marks.csv\")\n",
        "\n",
        "df_train = df[:300]\n",
        "\n",
        "X = np.array(df_train.drop([\"y\"], axis=1)).T\n",
        "y = np.array(df_train[\"y\"]).reshape(1, -1)\n",
        "\n",
        "# Defining our NN\n",
        "n0 = X.shape[0] #num of features\n",
        "n1 = 4 #num of neurons in hidden layer\n",
        "n2 = y.shape[0]\n",
        "layers = (n0, n1, n2)\n",
        "\n",
        "#print(n0, n1, n2)\n",
        "\n",
        "n_examples = X.shape[1]\n",
        "\n",
        "parameters = train_model(4500, X, y, layers, n_examples, lr=0.2, factor=0.01)\n",
        "\n",
        "print(parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl8xJ6Z_ye5H",
        "outputId": "db9101ed-ec0d-465c-e087-53f1b8d18210"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 cost:--> 0.691 accuracy--> 0.67\n",
            "iteration 1000 cost:--> 0.278 accuracy--> 0.88\n",
            "iteration 2000 cost:--> 0.215 accuracy--> 0.947\n",
            "iteration 3000 cost:--> 0.452 accuracy--> 0.823\n",
            "iteration 4000 cost:--> 0.206 accuracy--> 0.897\n",
            "iteration 4500 cost:--> 0.129 accuracy--> 0.933\n",
            "{'w1': array([[-1.195e-03, -3.654e-01,  1.147e+00],\n",
            "       [-1.305e-01, -1.780e-01,  1.073e+00],\n",
            "       [-3.036e-01,  3.260e-04,  1.036e+00],\n",
            "       [-2.828e-01, -2.660e-02,  1.040e+00]]), 'b1': array([[-7.243],\n",
            "       [-7.145],\n",
            "       [-6.857],\n",
            "       [-6.82 ]]), 'w2': array([[2.845, 2.782, 2.574, 2.565]]), 'b2': array([[-7.372]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "U4Ww6KFuL24z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df[300:]\n",
        "print(f\"Testing on {len(df_test)} data points\")\n",
        "\n",
        "X_test = np.array(df_test.drop([\"y\"], axis=1)).T # feature matrix\n",
        "y_test = np.array(df_test[\"y\"]).reshape(1, -1)\n",
        "\n",
        "#Defining size of our layers\n",
        "n0 = X_test.shape[0] #input size = number of features\n",
        "n1 = 4 # 4 neurons on the hidden layer\n",
        "n2 = y_test.shape[0] # one neuron for output layers\n",
        "\n",
        "n_examples = X_test.shape[1]\n",
        "\n",
        "#Tuple of our layers.\n",
        "layers = (n0, n1, n2)\n",
        "\n",
        "#Initialize the class for OurNeuralNetwork\n",
        "s = MyNeuralNet(X=X_test, y=y_test, layers=layers, m=n_examples, lr=0.2, factor=0.01)\n",
        "\n",
        "# Note the parameters in this case are parameters stored on the last iteration\n",
        "# of model training\n",
        "y_hat, layers_output = s.forward_propagation(parameters)\n",
        "\n",
        "# we replace with values greater or equal to 0.5 with 1 and 0 otherwise\n",
        "predictions = np.select(\n",
        "    [y_hat<0.5, y_hat>=0.5],      # list of conditions\n",
        "    [0, 1])# list of corresponding values or computations)\n",
        "# print the model output and the cost at each iteration.\n",
        "accuracy_score(y_true=np.squeeze(y_test), y_pred=np.squeeze(predictions), normalize=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UnawtoPL6tB",
        "outputId": "81ede65f-7bc7-4556-a2be-db1001160324"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on 95 data points\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8421052631578947"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}